import cv2
import math
import torch
from skimage.exposure import match_histograms
from einops import rearrange, repeat
import numpy as np
import py3d_tools as p3d
import os
import generator
from helpers import DepthModel, sampler_fn
import pandas as pd
import json
import random

class animator:

    def __int__(self, device, model_path):
        self.device = device
        self.model_path = model_path

    def next_seed(self, args):
        if args.seed_behavior == 'iter':
            args.seed += 1
        elif args.seed_behavior == 'fixed':
            pass  # always keep seed the same
        else:
            args.seed = random.randint(0, 2 ** 32 - 1)
        return args.seed

    def anim_frame_warp_2d(self, prev_img_cv2, args, anim_args, keys, frame_idx):
        angle = keys.angle_series[frame_idx]
        zoom = keys.zoom_series[frame_idx]
        translation_x = keys.translation_x_series[frame_idx]
        translation_y = keys.translation_y_series[frame_idx]

        center = (args.W // 2, args.H // 2)
        trans_mat = np.float32([[1, 0, translation_x], [0, 1, translation_y]])
        rot_mat = cv2.getRotationMatrix2D(center, angle, zoom)
        trans_mat = np.vstack([trans_mat, [0, 0, 1]])
        rot_mat = np.vstack([rot_mat, [0, 0, 1]])
        xform = np.matmul(rot_mat, trans_mat)

        return cv2.warpPerspective(
            prev_img_cv2,
            xform,
            (prev_img_cv2.shape[1], prev_img_cv2.shape[0]),
            borderMode=cv2.BORDER_WRAP if anim_args.border == 'wrap' else cv2.BORDER_REPLICATE
        )


    def anim_frame_warp_3d(self, prev_img_cv2, depth, anim_args, keys, frame_idx):
        TRANSLATION_SCALE = 1.0 / 200.0  # matches Disco
        translate_xyz = [
            -keys.translation_x_series[frame_idx] * TRANSLATION_SCALE,
            keys.translation_y_series[frame_idx] * TRANSLATION_SCALE,
            -keys.translation_z_series[frame_idx] * TRANSLATION_SCALE
        ]
        rotate_xyz = [
            math.radians(keys.rotation_3d_x_series[frame_idx]),
            math.radians(keys.rotation_3d_y_series[frame_idx]),
            math.radians(keys.rotation_3d_z_series[frame_idx])
        ]
        rot_mat = p3d.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=self.device), "XYZ").unsqueeze(0)
        result = self.transform_image_3d(prev_img_cv2, depth, rot_mat, translate_xyz, anim_args)
        torch.cuda.empty_cache()
        return result



    def add_noise(self, sample: torch.Tensor, noise_amt: float) -> torch.Tensor:
        return sample + torch.randn(sample.shape, device=sample.device) * noise_amt

    def sample_from_cv2(self, sample: np.ndarray) -> torch.Tensor:
        sample = ((sample.astype(float) / 255.0) * 2) - 1
        sample = sample[None].transpose(0, 3, 1, 2).astype(np.float16)
        sample = torch.from_numpy(sample)
        return sample


    def sample_to_cv2(self, sample: torch.Tensor, type=np.uint8) -> np.ndarray:
        sample_f32 = rearrange(sample.squeeze().cpu().numpy(), "c h w -> h w c").astype(np.float32)
        sample_f32 = ((sample_f32 * 0.5) + 0.5).clip(0, 1)
        sample_int8 = (sample_f32 * 255)
        return sample_int8.astype(type)

    def render_animation(self, args, anim_args, animation_prompts, half_precision=True):
        # animations use key framed prompts
        args.prompts = animation_prompts

        # expand key frame strings to values
        keys = DeformAnimKeys(anim_args)

        # resume animation
        start_frame = 0
        if anim_args.resume_from_timestring:
            for tmp in os.listdir(args.outdir):
                if tmp.split("_")[0] == anim_args.resume_timestring:
                    start_frame += 1
            start_frame = start_frame - 1

        # create output folder for the batch
        os.makedirs(args.outdir, exist_ok=True)
        print(f"Saving animation frames to {args.outdir}")

        # save settings for the batch
        settings_filename = os.path.join(args.outdir, f"{args.timestring}_settings.txt")
        with open(settings_filename, "w+", encoding="utf-8") as f:
            s = {**dict(args.__dict__), **dict(anim_args.__dict__)}
            json.dump(s, f, ensure_ascii=False, indent=4)

        # resume from timestring
        if anim_args.resume_from_timestring:
            args.timestring = anim_args.resume_timestring

        # expand prompts out to per-frame
        prompt_series = pd.Series([np.nan for a in range(anim_args.max_frames)])
        for i, prompt in animation_prompts.items():
            prompt_series[i] = prompt
        prompt_series = prompt_series.ffill().bfill()

        # check for video inits
        using_vid_init = anim_args.animation_mode == 'Video Input'

        # load depth model for 3D
        predict_depths = (anim_args.animation_mode == '3D' and anim_args.use_depth_warping) or anim_args.save_depth_maps
        if predict_depths:
            depth_model = DepthModel(self.device)
            depth_model.load_midas(self.model_path)
            if anim_args.midas_weight < 1.0:
                depth_model.load_adabins()
        else:
            depth_model = None
            anim_args.save_depth_maps = False

        # state for interpolating between diffusion steps
        turbo_steps = 1 if using_vid_init else int(anim_args.diffusion_cadence)
        turbo_prev_image, turbo_prev_frame_idx = None, 0
        turbo_next_image, turbo_next_frame_idx = None, 0

        # resume animation
        prev_sample = None
        color_match_sample = None
        if anim_args.resume_from_timestring:
            last_frame = start_frame - 1
            if turbo_steps > 1:
                last_frame -= last_frame % turbo_steps
            path = os.path.join(args.outdir, f"{args.timestring}_{last_frame:05}.png")
            img = cv2.imread(path)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            prev_sample = self.sample_from_cv2(img)
            if anim_args.color_coherence != 'None':
                color_match_sample = img
            if turbo_steps > 1:
                turbo_next_image, turbo_next_frame_idx = self.sample_to_cv2(prev_sample, type=np.float32), last_frame
                turbo_prev_image, turbo_prev_frame_idx = turbo_next_image, turbo_next_frame_idx
                start_frame = last_frame + turbo_steps

        args.n_samples = 1
        frame_idx = start_frame
        while frame_idx < anim_args.max_frames:
            print(f"Rendering animation frame {frame_idx} of {anim_args.max_frames}")
            noise = keys.noise_schedule_series[frame_idx]
            strength = keys.strength_schedule_series[frame_idx]
            contrast = keys.contrast_schedule_series[frame_idx]
            depth = None

            # emit in-between frames
            if turbo_steps > 1:
                tween_frame_start_idx = max(0, frame_idx - turbo_steps)
                for tween_frame_idx in range(tween_frame_start_idx, frame_idx):
                    tween = float(tween_frame_idx - tween_frame_start_idx + 1) / float(frame_idx - tween_frame_start_idx)
                    print(f"  creating in between frame {tween_frame_idx} tween:{tween:0.2f}")

                    advance_prev = turbo_prev_image is not None and tween_frame_idx > turbo_prev_frame_idx
                    advance_next = tween_frame_idx > turbo_next_frame_idx

                    if depth_model is not None:
                        assert (turbo_next_image is not None)
                        depth = depth_model.predict(turbo_next_image, anim_args)

                    if anim_args.animation_mode == '2D':
                        if advance_prev:
                            turbo_prev_image = self.anim_frame_warp_2d(turbo_prev_image, args, anim_args, keys, tween_frame_idx)
                        if advance_next:
                            turbo_next_image = self.anim_frame_warp_2d(turbo_next_image, args, anim_args, keys, tween_frame_idx)
                    else:  # '3D'
                        if advance_prev:
                            turbo_prev_image = self.anim_frame_warp_3d(turbo_prev_image, depth, anim_args, keys, tween_frame_idx)
                        if advance_next:
                            turbo_next_image = self.anim_frame_warp_3d(turbo_next_image, depth, anim_args, keys, tween_frame_idx)
                    turbo_prev_frame_idx = turbo_next_frame_idx = tween_frame_idx

                    if turbo_prev_image is not None and tween < 1.0:
                        img = turbo_prev_image * (1.0 - tween) + turbo_next_image * tween
                    else:
                        img = turbo_next_image

                    filename = f"{args.timestring}_{tween_frame_idx:05}.png"
                    cv2.imwrite(os.path.join(args.outdir, filename), cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2BGR))
                    if anim_args.save_depth_maps:
                        depth_model.save(os.path.join(args.outdir, f"{args.timestring}_depth_{tween_frame_idx:05}.png"),
                                         depth)
                if turbo_next_image is not None:
                    prev_sample = self.sample_from_cv2(turbo_next_image)

            # apply transforms to previous frame
            if prev_sample is not None:
                if anim_args.animation_mode == '2D':
                    prev_img = self.anim_frame_warp_2d(self.sample_to_cv2(prev_sample), args, anim_args, keys, frame_idx)
                else:  # '3D'
                    prev_img_cv2 = self.sample_to_cv2(prev_sample)
                    depth = depth_model.predict(prev_img_cv2, anim_args) if depth_model else None
                    prev_img = self.anim_frame_warp_3d(prev_img_cv2, depth, anim_args, keys, frame_idx)

                # apply color matching
                if anim_args.color_coherence != 'None':
                    if color_match_sample is None:
                        color_match_sample = prev_img.copy()
                    else:
                        prev_img = self.maintain_colors(prev_img, color_match_sample, anim_args.color_coherence)

                # apply scaling
                contrast_sample = prev_img * contrast
                # apply frame noising
                noised_sample = self.add_noise(self.sample_from_cv2(contrast_sample), noise)

                # use transformed previous frame as init for current
                args.use_init = True
                if half_precision:
                    args.init_sample = noised_sample.half().to(self.device)
                else:
                    args.init_sample = noised_sample.to(self.device)
                args.strength = max(0.0, min(1.0, strength))

            # grab prompt for current frame
            args.prompt = prompt_series[frame_idx]
            print(f"{args.prompt} {args.seed}")

            # grab init image for current frame
            if using_vid_init:
                init_frame = os.path.join(args.outdir, 'inputframes', f"{frame_idx + 1:04}.jpg")
                print(f"Using video init frame {init_frame}")
                args.init_image = init_frame

            # sample the diffusion model
            sample, image = generator.generate(args, return_latent=False, return_sample=True)
            if not using_vid_init:
                prev_sample = sample

            if turbo_steps > 1:
                turbo_prev_image, turbo_prev_frame_idx = turbo_next_image, turbo_next_frame_idx
                turbo_next_image, turbo_next_frame_idx = self.sample_to_cv2(sample, type=np.float32), frame_idx
                frame_idx += turbo_steps
            else:
                filename = f"{args.timestring}_{frame_idx:05}.png"
                image.save(os.path.join(args.outdir, filename))
                if anim_args.save_depth_maps:
                    if depth is None:
                        depth = depth_model.predict(self.sample_to_cv2(sample), anim_args)
                    depth_model.save(os.path.join(args.outdir, f"{args.timestring}_depth_{frame_idx:05}.png"), depth)
                frame_idx += 1

            #display.clear_output(wait=True)
            #display.display(image)

            args.seed = self.next_seed(args)


    def maintain_colors(self, prev_img, color_match_sample, mode):
        if mode == 'Match Frame 0 RGB':
            return match_histograms(prev_img, color_match_sample, multichannel=True)
        elif mode == 'Match Frame 0 HSV':
            prev_img_hsv = cv2.cvtColor(prev_img, cv2.COLOR_RGB2HSV)
            color_match_hsv = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2HSV)
            matched_hsv = match_histograms(prev_img_hsv, color_match_hsv, multichannel=True)
            return cv2.cvtColor(matched_hsv, cv2.COLOR_HSV2RGB)
        else:  # Match Frame 0 LAB
            prev_img_lab = cv2.cvtColor(prev_img, cv2.COLOR_RGB2LAB)
            color_match_lab = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2LAB)
            matched_lab = match_histograms(prev_img_lab, color_match_lab, multichannel=True)
            return cv2.cvtColor(matched_lab, cv2.COLOR_LAB2RGB)



    def transform_image_3d(self, prev_img_cv2, depth_tensor, rot_mat, translate, anim_args):
        # adapted and optimized version of transform_image_3d from Disco Diffusion https://github.com/alembics/disco-diffusion
        w, h = prev_img_cv2.shape[1], prev_img_cv2.shape[0]

        aspect_ratio = float(w) / float(h)
        near, far, fov_deg = anim_args.near_plane, anim_args.far_plane, anim_args.fov
        persp_cam_old = p3d.FoVPerspectiveCameras(near, far, aspect_ratio, fov=fov_deg, degrees=True, device=self.device)
        persp_cam_new = p3d.FoVPerspectiveCameras(near, far, aspect_ratio, fov=fov_deg, degrees=True, R=rot_mat,
                                                  T=torch.tensor([translate]), device=self.device)

        # range of [-1,1] is important to torch grid_sample's padding handling
        y, x = torch.meshgrid(torch.linspace(-1., 1., h, dtype=torch.float32, device=self.device),
                              torch.linspace(-1., 1., w, dtype=torch.float32, device=self.device))
        z = torch.as_tensor(depth_tensor, dtype=torch.float32, device=self.device)
        xyz_old_world = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1)

        xyz_old_cam_xy = persp_cam_old.get_full_projection_transform().transform_points(xyz_old_world)[:, 0:2]
        xyz_new_cam_xy = persp_cam_new.get_full_projection_transform().transform_points(xyz_old_world)[:, 0:2]

        offset_xy = xyz_new_cam_xy - xyz_old_cam_xy
        # affine_grid theta param expects a batch of 2D mats. Each is 2x3 to do rotation+translation.
        identity_2d_batch = torch.tensor([[1., 0., 0.], [0., 1., 0.]], device=self.device).unsqueeze(0)
        # coords_2d will have shape (N,H,W,2).. which is also what grid_sample needs.
        coords_2d = torch.nn.functional.affine_grid(identity_2d_batch, [1, 1, h, w], align_corners=False)
        offset_coords_2d = coords_2d - torch.reshape(offset_xy, (h, w, 2)).unsqueeze(0)

        image_tensor = rearrange(torch.from_numpy(prev_img_cv2.astype(np.float32)), 'h w c -> c h w').to(self.device)
        new_image = torch.nn.functional.grid_sample(
            image_tensor.add(1 / 512 - 0.0001).unsqueeze(0),
            offset_coords_2d,
            mode=anim_args.sampling_mode,
            padding_mode=anim_args.padding_mode,
            align_corners=False
        )

        # convert back to cv2 style numpy array
        result = rearrange(
            new_image.squeeze().clamp(0, 255),
            'c h w -> h w c'
        ).cpu().numpy().astype(prev_img_cv2.dtype)
        return result
